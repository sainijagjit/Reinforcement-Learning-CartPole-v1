{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HILL CLIMBING ALGORITHIM A CART POLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(4,)\n",
      "Action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from gym.envs.registration import register\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import adam\n",
    "\n",
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "#for continuous action space\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size: 2 , State size: 4\n",
      "WARNING:tensorflow:From C:\\Users\\jagjit.singh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jagjit.singh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jagjit.singh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jagjit.singh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class QNAgent():\n",
    "    def __init__(self,env,discount_rate=0.97,learning_rate=0.01,randomness=1,qtable1=0):\n",
    "        self.action_size = env.action_space.n\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        print(\"Action size: {} , State size: {}\".format(self.action_size,self.state_size))\n",
    "        \n",
    "        #our model was not exploring other actions, so fix\n",
    "        #probability of exploring priortizing exploratory action over the policy action\n",
    "        self.eps=1.0\n",
    "        \n",
    "        self.discount_rate=discount_rate\n",
    "        self.learning_rate=learning_rate\n",
    "        self.randomness=randomness\n",
    "\n",
    "   \n",
    "        self.build_model()\n",
    "        self.replay_buffer = deque(maxlen=10000000)\n",
    "        self.random_count=0\n",
    "        self.policy_count=0\n",
    "            \n",
    "    #defining the member function\n",
    "    #defining rows as state and coloumns as actions\n",
    "    def build_model(self):\n",
    "        \n",
    "        self.model =Sequential()\n",
    "        self.model.add(Dense(units=24,activation='relu',input_dim=4))\n",
    "        self.model.add(Dense(units=24,activation='relu'))\n",
    "        self.model.add(Dense(units=self.action_size,activation='linear'))\n",
    "        self.model.compile(loss='mse',optimizer=adam(lr=self.learning_rate))\n",
    "                  \n",
    "\n",
    "    #overwriting the get action function\n",
    "    def get_action(self,state,ep,i):\n",
    "        \n",
    "        q_state = self.model.predict(state)\n",
    "        #to perform the explortory actions, first we are storing the obvious action\n",
    "        action_greedy= np.argmax(q_state[0])\n",
    "        #here we are storing the random action, which is defined in the parent class\n",
    "        action_random=random.choice(range(self.action_size))\n",
    "                       \n",
    "        #below retuens which position is max\n",
    "        \n",
    "        #1. calculating a random value\n",
    "        #2. if it is less than 1 (that we will decay exponentilly)\n",
    "        \n",
    "        if self.randomness==1:\n",
    "            kk=random.random()\n",
    "            if kk<self.eps:\n",
    "                #print('random')\n",
    "                self.random_count+=1\n",
    "                return action_random\n",
    "            else:\n",
    "                #print('Policy')\n",
    "                self.policy_count+=1\n",
    "                return action_greedy\n",
    "\n",
    "        else:\n",
    "            return action_greedy\n",
    "    \n",
    "    def train(self,experience,reached,batch_size= 50):\n",
    "        #if experience[4]:\n",
    "        #    self.eps=self.eps*0.99\n",
    "        #self.replay_buffer.append(experience)\n",
    "        #if len(self.replay_buffer) < batch_size: \n",
    "            #print('The buffer lenght is {}'.format(len(self.replay_buffer)))\n",
    "         #   return\n",
    "        #samples = random.sample(self.replay_buffer,batch_size) \n",
    "        # samples.append(experience)\n",
    "        samples=experience\n",
    "        if reached:\n",
    "            gamma=1\n",
    "        else:\n",
    "            gamma=0.99\n",
    "        reward_cumm=0\n",
    "        for state, action, state_next, reward, done in reversed(samples):\n",
    "    \n",
    "           \n",
    "            reward_cumm = reward_cumm + reward*gamma\n",
    "            q_target = reward_cumm\n",
    "            q_next =  self.model.predict(state_next)\n",
    "            q_value = self.model.predict(state)\n",
    "            if not done:\n",
    "                q_target= reward_cumm + self.discount_rate*np.max(q_next[0])\n",
    "  \n",
    "            q_value[0][action] = q_target\n",
    "            #print(\"q_target before: {} , q_target after: {}\".format(q_target,q_value[0][action]))\n",
    "            if not reached:\n",
    "                gamma=gamma*0.99\n",
    "            \n",
    "            \n",
    "            self.model.fit(state, q_value, verbose=0)\n",
    "        self.eps=self.eps*0.99\n",
    "\n",
    "                #print('before {} after {}'.format(kk,self.eps))\n",
    "\n",
    "\n",
    "        \n",
    "agent= QNAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jagjit.singh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jagjit.singh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "Episode: 0, Total reward: 25.0, eps: 1.0, Random Count: 25,Policy Count: 0\n",
      "Episode: 1, Total reward: 11.0, eps: 0.99, Random Count: 11,Policy Count: 0\n",
      "Episode: 2, Total reward: 12.0, eps: 0.9801, Random Count: 12,Policy Count: 0\n",
      "Episode: 3, Total reward: 19.0, eps: 0.9702989999999999, Random Count: 19,Policy Count: 0\n",
      "Episode: 4, Total reward: 27.0, eps: 0.96059601, Random Count: 25,Policy Count: 2\n",
      "Episode: 5, Total reward: 35.0, eps: 0.9509900498999999, Random Count: 32,Policy Count: 3\n",
      "Episode: 6, Total reward: 15.0, eps: 0.9414801494009999, Random Count: 13,Policy Count: 2\n",
      "Episode: 7, Total reward: 40.0, eps: 0.9320653479069899, Random Count: 38,Policy Count: 2\n",
      "Episode: 8, Total reward: 19.0, eps: 0.92274469442792, Random Count: 19,Policy Count: 0\n",
      "Episode: 9, Total reward: 17.0, eps: 0.9135172474836407, Random Count: 14,Policy Count: 3\n",
      "Episode: 10, Total reward: 23.0, eps: 0.9043820750088043, Random Count: 20,Policy Count: 3\n",
      "Episode: 11, Total reward: 10.0, eps: 0.8953382542587163, Random Count: 9,Policy Count: 1\n",
      "Episode: 12, Total reward: 20.0, eps: 0.8863848717161291, Random Count: 17,Policy Count: 3\n",
      "Episode: 13, Total reward: 9.0, eps: 0.8775210229989678, Random Count: 7,Policy Count: 2\n",
      "Episode: 14, Total reward: 25.0, eps: 0.8687458127689781, Random Count: 22,Policy Count: 3\n",
      "Episode: 15, Total reward: 28.0, eps: 0.8600583546412883, Random Count: 26,Policy Count: 2\n",
      "Episode: 16, Total reward: 13.0, eps: 0.8514577710948754, Random Count: 10,Policy Count: 3\n",
      "Episode: 17, Total reward: 28.0, eps: 0.8429431933839266, Random Count: 21,Policy Count: 7\n",
      "Episode: 18, Total reward: 12.0, eps: 0.8345137614500874, Random Count: 11,Policy Count: 1\n",
      "Episode: 19, Total reward: 41.0, eps: 0.8261686238355865, Random Count: 34,Policy Count: 7\n",
      "Episode: 20, Total reward: 9.0, eps: 0.8179069375972307, Random Count: 9,Policy Count: 0\n",
      "Episode: 21, Total reward: 30.0, eps: 0.8097278682212583, Random Count: 26,Policy Count: 4\n",
      "Episode: 22, Total reward: 15.0, eps: 0.8016305895390458, Random Count: 13,Policy Count: 2\n",
      "Episode: 23, Total reward: 35.0, eps: 0.7936142836436553, Random Count: 27,Policy Count: 8\n",
      "Episode: 24, Total reward: 16.0, eps: 0.7856781408072188, Random Count: 13,Policy Count: 3\n",
      "Episode: 25, Total reward: 32.0, eps: 0.7778213593991465, Random Count: 28,Policy Count: 4\n",
      "Episode: 26, Total reward: 10.0, eps: 0.7700431458051551, Random Count: 5,Policy Count: 5\n",
      "Episode: 27, Total reward: 9.0, eps: 0.7623427143471035, Random Count: 5,Policy Count: 4\n",
      "Episode: 28, Total reward: 18.0, eps: 0.7547192872036325, Random Count: 12,Policy Count: 6\n",
      "Episode: 29, Total reward: 15.0, eps: 0.7471720943315961, Random Count: 10,Policy Count: 5\n",
      "Episode: 30, Total reward: 25.0, eps: 0.7397003733882802, Random Count: 19,Policy Count: 6\n",
      "Episode: 31, Total reward: 21.0, eps: 0.7323033696543974, Random Count: 15,Policy Count: 6\n",
      "Episode: 32, Total reward: 18.0, eps: 0.7249803359578534, Random Count: 13,Policy Count: 5\n",
      "Episode: 33, Total reward: 19.0, eps: 0.7177305325982748, Random Count: 11,Policy Count: 8\n",
      "Episode: 34, Total reward: 33.0, eps: 0.7105532272722921, Random Count: 26,Policy Count: 7\n",
      "Episode: 35, Total reward: 14.0, eps: 0.7034476949995692, Random Count: 7,Policy Count: 7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5dee1f30eff7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mtotal_reward\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m#for displaying the popup window\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mi\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m#clear_output(wait=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001b[0m in \u001b[0;36mflip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_mouse_cursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyglet\\gl\\win32.py\u001b[0m in \u001b[0;36mflip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0m_gdi32\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSwapBuffers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhdc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vsync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_reward=0\n",
    "for ep in range(500):\n",
    "    state = env.reset()\n",
    "    done=False\n",
    "    total_reward=0\n",
    "    i=0\n",
    "    episode_training=[]\n",
    "    while not done:\n",
    "    #   action = env.action_space.sample()\n",
    "        state = np.reshape(state,[1,4])\n",
    "        action = agent.get_action(state,ep,i)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = np.reshape(next_state,[1,4])\n",
    "        \n",
    "        \n",
    "        episode_training.append((state,action,next_state,reward,done))\n",
    "        #print(done)\n",
    "        state=next_state\n",
    "        total_reward+=reward\n",
    "        #for displaying the popup window\n",
    "        env.render()\n",
    "        i+=1\n",
    "        #clear_output(wait=True)\n",
    "    print(\"Episode: {}, Total reward: {}, eps: {}, Random Count: {},Policy Count: {}\".format(ep,total_reward,agent.eps,agent.random_count,agent.policy_count))\n",
    "    if i>=200:\n",
    "        agent.train(episode_training,True)\n",
    "    else:\n",
    "        agent.train(episode_training,False)\n",
    "    agent.random_count=0\n",
    "    agent.policy_count=0\n",
    "        \n",
    "agent.model.save(\"model.h5\")   \n",
    "        \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
